<html lang="en"><head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="/static/home/main.css">

    
    <title>ROSE Lab </title>
    
</head>

<body>
    <div class="container mt-3 mb-2">
        <img src="/static/home/logo.png" alt="">
    </div>
    <header class="site-header">
        <nav class="navbar navbar-expand-md navbar-dark bg-steel pt-0 pb-0">
            <div class="container">
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle" aria-controls="navbarToggle" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarToggle">
                    <div class="navbar-nav mr-auto">
                        <a class="nav-item nav-link" href="/">Home</a>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Event</a>
                            <div class="dropdown-menu">
                                
                                <a class="dropdown-item" href="/event/VISVA2014/">VISVA 2014</a>
                                
                                <a class="dropdown-item" href="/event/VISVA2016/">VISVA 2016</a>
                                
                                <a class="dropdown-item" href="/event/VISVA2017/">VISVA 2017</a>
                                
                                <a class="dropdown-item" href="/event/VISVA2018/">VISVA 2018</a>
                                
                                <a class="dropdown-item" href="/event/VISVA2020/">VISVA 2020</a>
                                
                            </div>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Challenge</a>
                            <div class="dropdown-menu">
                                
                                <a class="dropdown-item" href="/challenge/ActionRecognitionChallenge/">Action Recognition Challenge</a>
                                
                                <a class="dropdown-item" href="/challenge/WeMageChallenge2014/">WeMage Challenge 2014</a>
                                
                            </div>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Dataset</a>
                            <div class="dropdown-menu">
                                
                                <a class="dropdown-item" href="/dataset/recapturedImages/">Recaptured Images Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/videoObjectInstance/">Video Object Instance Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/actionRecognition/">Action Recognition Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/sir2Benchmark/">SIR2 Benchmark Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/faceLivenessDetection/">ROSE-Youtu Face Liveness Detection Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/cctvFights/">NTU CCTV-Fights Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/Warwick-NTU/">Warwick-NTU Multi-camera Forecasting (WNMF) database</a>
                                
                                <a class="dropdown-item" href="/dataset/SIXray-D/">SIXray-D Annotations</a>
                                
                            </div>
                        </li>
                        <a class="nav-item nav-link" href="/contact/">Contact</a>
                    </div>
                    <!-- Navbar Right Side -->
                    <div class="navbar-nav">
                        
                        <a class="nav-item nav-link" href="/login/">Login/Signup</a>
                        
                    </div>
                </div>
            </div>
        </nav>
    </header>

    <main role="main" class="container mt-2">
        
        
    <div class="media-body">
        <div class="article-content"><h2>Action Recognition Datasets: "NTU RGB+D" Dataset and "NTU RGB+D 120" Dataset</h2>

<h2><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><strong><span style="font-size:12.0pt">(</span></strong><span style="font-size:12.0pt">also include </span><strong><span style="font-size:12.0pt">AUTH UAV Gesture Dataset: NTU 4-Class)</span></strong></span></span></h2>

<p>This page introduces two datasets: "NTU RGB+D" and "NTU RGB+D 120".<br>
"NTU RGB+D" contains 60 action classes and 56,880 video samples.<br>
"NTU RGB+D 120" extends "NTU RGB+D" by adding another 60 classes and another 57,600 video samples, i.e., "NTU RGB+D 120" has 120 classes and 114,480 samples in total.<br>
These two datasets both contain RGB videos, depth map sequences, 3D skeletal data, and infrared (IR) videos for each sample. Each dataset is captured by three Kinect V2 cameras concurrently.<br>
The resolutions of RGB videos are 1920x1080, depth maps and IR videos are all in 512x424, and 3D skeletal data contains the 3D coordinates of 25 body joints at each frame.</p>

<p>&nbsp;</p>

<h3>1. Action Classes</h3>

<p>The actions in these two datasets are in three major categories: daily actions, mutual actions, and medical conditions, as shown in the tables below.<br>
<strong>Note: actions labelled from A1 to A60 are contained in "NTU RGB+D", and actions labelled from A1 to A120 are in "NTU RGB+D 120".</strong></p>

<p>&nbsp;</p>

<p><strong>1.1 Dataset in number(82)</strong></p>



<table align="center" border="1" style="width:92%">
	<tbody>
		<tr>
			<td>A1: drink water</td>
			<td>A2: eat meal</td>
			<td>A3: brush teeth</td>
			<td>A4: brush hair</td>
		</tr>
		<tr>
			<td>A5: drop</td>
			<td>A6: pick up</td>
			<td>A7: throw</td>
			<td>A8: sit down</td>
		</tr>
		<tr>
			<td>A9: stand up</td>
			<td>A10: clapping</td>
			<td>A11: reading</td>
			<td>A12: writing</td>
		</tr>
		<tr>
			<td>A13: tear up paper</td>
			<td>A14: put on jacket</td>
			<td>A15: take off jacket</td>
			<td>A16: put on a shoe</td>
		</tr>
		<tr>
			<td>A17: take off a shoe</td>
			<td>A18: put on glasses</td>
			<td>A19: take off glasses</td>
			<td>A20: put on a hat/cap</td>
		</tr>
		<tr>
			<td>A21: take off a hat/cap</td>
			<td>A22: cheer up</td>
			<td>A23: hand waving</td>
			<td>A24: kicking something</td>
		</tr>
		<tr>
			<td>A25: reach into pocket</td>
			<td>A26: hopping</td>
			<td>A27: jump up</td>
			<td>A28: phone call</td>
		</tr>
		<tr>
			<td>A29: play with phone/tablet</td>
			<td>A30: type on a keyboard</td>
			<td>A31: point to something</td>
			<td>A32: taking a selfie</td>
		</tr>
		<tr>
			<td>A33: check time (from watch)</td>
			<td>A34: rub two hands</td>
			<td>A35: nod head/bow</td>
			<td>A36: shake head</td>
		</tr>
		<tr>
			<td>A37: wipe face</td>
			<td>A38: salute</td>
			<td>A39: put palms together</td>
			<td>A40: cross hands in front</td>
		</tr>
		<tr>
			<td>A61: put on headphone</td>
			<td>A62: take off headphone</td>
			<td>A63: shoot at basket</td>
			<td>A64: bounce ball</td>
		</tr>
		<tr>
			<td>A65: tennis bat swing</td>
			<td>A66: juggle table tennis ball</td>
			<td>A67: hush</td>
			<td>A68: flick hair</td>
		</tr>
		<tr>
			<td>A69: thumb up</td>
			<td>A70: thumb down</td>
			<td>A71: make OK sign</td>
			<td>A72: make victory sign</td>
		</tr>
		<tr>
			<td>A73: staple book</td>
			<td>A74: counting money</td>
			<td>A75: cutting nails</td>
			<td>A76: cutting paper</td>
		</tr>
		<tr>
			<td>A77: snap fingers</td>
			<td>A78: open bottle</td>
			<td>A79: sniff/smell</td>
			<td>A80: squat down</td>
		</tr>
		<tr>
			<td>A81: toss a coin</td>
			<td>A82: fold paper</td>
			<td>A83: ball up paper</td>
			<td>A84: play magic cube</td>
		</tr>
		<tr>
			<td>A85: apply cream on face</td>
			<td>A86: apply cream on hand</td>
			<td>A87: put on bag</td>
			<td>A88: take off bag</td>
		</tr>
		<tr>
			<td>A89: put object into bag</td>
			<td>A90: take object out of bag</td>
			<td>A91: open a box</td>
			<td>A92: move heavy objects</td>
		</tr>
		<tr>
			<td>A93: shake fist</td>
			<td>A94: throw up cap/hat</td>
			<td>A95: capitulate</td>
			<td>A96: cross arms</td>
		</tr>
		<tr>
			<td>A97: arm circles</td>
			<td>A98: arm swings</td>
			<td>A99: run on the spot</td>
			<td>A100: butt kicks</td>
		</tr>
		<tr>
			<td>A101: cross toe touch</td>
			<td>A102: side kick</td>
			<td>-</td>
			<td>-</td>
		</tr>
	</tbody>
</table>

<p>&nbsp;</p>

<p><strong>1.2 Medical Conditions (12)</strong></p>

<table align="center" border="1" style="width:92%">
	<tbody>
		<tr>
			<td>A41: sneeze/cough</td>
			<td>A42: staggering</td>
			<td>A43: falling down</td>
			<td>A44: headache</td>
		</tr>
		<tr>
			<td>A45: chest pain</td>
			<td>A46: back pain</td>
			<td>A47: neck pain</td>
			<td>A48: nausea/vomiting</td>
		</tr>
		<tr>
			<td>A49: fan self</td>
			<td>A103: yawn</td>
			<td>A104: stretch oneself</td>
			<td>A105: blow nose</td>
		</tr>
	</tbody>
</table>

<p>&nbsp;</p>

<p><strong>1.3 Mutual Actions / Two Person Interactions (26)</strong></p>

<table align="center" border="1" style="width:92%">
	<tbody>
		<tr>
			<td>A50: punch/slap</td>
			<td>A51: kicking</td>
			<td>A52: pushing</td>
			<td>A53: pat on back</td>
		</tr>
		<tr>
			<td>A54: point finger</td>
			<td>A55: hugging</td>
			<td>A56: giving object</td>
			<td>A57: touch pocket</td>
		</tr>
		<tr>
			<td>A58: shaking hands</td>
			<td>A59: walking towards</td>
			<td>A60: walking apart</td>
			<td>A106: hit with object</td>
		</tr>
		<tr>
			<td>A107: wield knife</td>
			<td>A108: knock over</td>
			<td>A109: grab stuff</td>
			<td>A110: shoot with gun</td>
		</tr>
		<tr>
			<td>A111: step on foot</td>
			<td>A112: high-five</td>
			<td>A113: cheers and drink</td>
			<td>A114: carry object</td>
		</tr>
		<tr>
			<td>A115: take a photo</td>
			<td>A116: follow</td>
			<td>A117: whisper</td>
			<td>A118: exchange things</td>
		</tr>
		<tr>
			<td>A119: support somebody</td>
			<td>A120: rock-paper-scissors</td>
			<td>-</td>
			<td>-</td>
		</tr>
	</tbody>
</table>

<p>&nbsp;</p>

<p>&nbsp;</p>

<h3>2. Size of Datasets</h3>

<p>To ease the downloading, we separate the modalities of the samples into different files. The size of each modality is shown in the below table:</p>

<table align="center" border="1" style="width:75%">
	<tbody>
		<tr>
			<td><strong>Data Modality</strong></td>
			<td><strong>"NTU RGB+D"</strong></td>
			<td><strong>"NTU RGB+D 120"</strong></td>
		</tr>
		<tr>
			<td>3D skeletons (body joints)</td>
			<td>5.8 GB</td>
			<td>5.8+4.5 GB</td>
		</tr>
		<tr>
			<td>Masked depth maps*</td>
			<td>83 GB</td>
			<td>83+64 GB</td>
		</tr>
		<tr>
			<td>Full depth maps</td>
			<td>886 GB</td>
			<td>886+549 GB</td>
		</tr>
		<tr>
			<td>RGB videos</td>
			<td>136 GB</td>
			<td>136+124 GB</td>
		</tr>
		<tr>
			<td>IR data</td>
			<td>221 GB</td>
			<td>221+168 GB</td>
		</tr>
		<tr>
			<td><strong>Total</strong></td>
			<td><strong>1.3 TB</strong></td>
			<td><strong>2.3 TB</strong></td>
		</tr>
	</tbody>
</table>

<p>&nbsp;</p>

<p>*Masked depth maps are the foreground masked version of the depth maps. Masking is done based on the locations of the detected body joints, to remove the background and less important parts of the depth maps and to improve the compression rate.</p>

<p>&nbsp;</p>

<h3>3. How to Download Datasets</h3>

<p>Researchers can register an account, submit the request form and accept the Release Agrement. We will validate your request and grand approve for downloading the datasets.The LoginID can be used for both "NTU RGB+D" and "NTU RGB+D 120".</p>

<p>&nbsp;</p>

<h3>4. More Information (FAQs and Sample Codes)</h3>

<p>We provide more information about the data, answers to FAQs, samples codes to read the data, and the latest published results on our datasets <a href="https://github.com/shahroudy/NTURGB-D" target="_blank">here</a>.</p>

<p>&nbsp;</p>

<h3>5. Sample Videos</h3>

<p><img alt="" src="/media/website/actionRecognition120.jpg" style="float:left; height:551px; width:1061px"></p>

<p>&nbsp;</p>

<table align="center">
	<tbody>
		<tr>
			<td><iframe frameborder="0" height="200" src="https://www.youtube.com/embed/O5WknxnwUoE?rel=0" width="320"></iframe>​​</td>
			<td><iframe frameborder="0" height="200" src="https://www.youtube.com/embed/dWs9aqgfgQ8?rel=0" width="320"></iframe></td>
		</tr>
		<tr>
			<td><iframe frameborder="0" height="200" src="https://www.youtube.com/embed/R3-7_XWf4nw?rel=0" width="320"></iframe></td>
			<td><iframe frameborder="0" height="200" src="https://www.youtube.com/embed/DmKgO2Pb7rQ?rel=0" width="320"></iframe></td>
		</tr>
	</tbody>
</table>

<h3>6. <u>Terms &amp; Conditions of Use</u></h3>

<p>The datasets are released for academic research only, and are free to researchers from educational or research institutes for non-commercial purposes.</p>

<p>The use of these two datasets is governed by the following terms and conditions:<br>
• Without the expressed permission of the ROSE Lab, any of the following will be considered <strong>illegal</strong>: redistribution, derivation or generation of a new dataset from this dataset, and commercial usage of any of these datasets in any way or form, either partially or in its entirety.<br>
• For the sake of privacy, images of all subjects in any of these datasets are only allowed for the demonstration in academic publications and presentations.<br>
• All users of "NTU RGB+D" and "NTU RGB+D 120" action recognition datasets agree to indemnify, defend and hold harmless, the ROSE Lab and its officers, employees, and agents, individually and collectively, from any and all losses, expenses, and damages.</p>

<p><span style="font-size:11pt"><span style="font-family:Calibri,sans-serif"><span style="color:red">If interested, researchers can register for an account, submit the request form and accept the Release Agreement. We will validate your request and grant approval&nbsp;for downloading the datasets.The LoginID can be used for both "NTU RGB+D" and "NTU RGB+D 120".</span></span></span></p>

<h3>7. <u>Related Publications</u></h3>

<p>All publications using "NTU RGB+D" or "NTU RGB+D 120" Action Recognition Database or any of the derived datasets(see Section 8) should include the following acknowledgement: "(Portions of) the research in this paper used the NTU RGB+D (or NTU RGB+D 120) Action Recognition Dataset made available by the ROSE Lab at the Nanyang Technological University, Singapore."<br>
<br>
<strong>Furthermore, these publications should cite the following papers:</strong></p>

<ul>
	<li><strong>Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang, "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis", IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016</strong> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.pdf">[PDF]</a>.</li>
	<li><strong>Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, Alex C. Kot, "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding", IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019.</strong> <a href="https://arxiv.org/pdf/1905.04757.pdf">[PDF]</a>.</li>
</ul>

<p>&nbsp;</p>

<p>Some related works on RGB+D action recognition:</p>

<ul>
	<li>Amir Shahroudy, Tian-Tsong Ng, Qingxiong Yang, Gang Wang, "Multimodal Multipart Learning for Action Recognition in Depth Videos", TPAMI, 2016.</li>
	<li>Amir Shahroudy, Tian-Tsong Ng, Yihong Gong, Gang Wang, "Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos" TPAMI, 2018.</li>
	<li>Amir Shahroudy, Gang Wang, Tian Tsong Ng, "Multi-modal Feature Fusion for Action Recognition in RGB-D Sequences", ISCCSP, 2014.</li>
	<li>Jun Liu, Amir Shahroudy, Dong Xu, Gang Wang, "Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition", ECCV, 2016.</li>
	<li>Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, Alex C. Kot, "Global Context-Aware Attention LSTM Networks for 3D Action Recognition", CVPR, 2017.</li>
	<li>Jun Liu, Amir Shahroudy, Dong Xu, Alex C. Kot, Gang Wang, "Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates", TPAMI, 2018.</li>
	<li>Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva, Alex C. Kot, "Skeleton-Based Human Action Recognition with Global Context-aware Attention LSTM Networks", TIP, 2018.</li>
	<li>Jun Liu, Amir Shahroudy, Gang Wang, Ling-Yu Duan, Alex C. Kot, "Skeleton-Based Online Action Prediction Using Scale Selection Network", TPAMI, 2019.</li>
	<li>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, and Alex Kot, "Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-Order Feature Analysis", ECCV 2020.</li>
	<li>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, and Alex Kot, "Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning", ICCV 2021.<br>
	&nbsp;</li>
</ul>

<h3>8. Derived Works Based on NTU RGB+D Dataset</h3>

<p>Below are some datasets that are derived from, or make partial use of, NTU RGB+D dataset:</p>

<p><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><strong><span style="font-size:12.0pt">8.1.&nbsp; LSMB19: A Large-Scale Motion Benchmark for Searching and Annotating in Continuous Motion Data Streams</span></strong><span style="font-size:12.0pt"> (</span><a href="http://mocap.fi.muni.cz/LSMB" style="color:blue; text-decoration:underline" target="_blank"><span style="font-size:12.0pt">http://mocap.fi.muni.cz/LSMB</span></a><span style="font-size:12.0pt">).</span></span></span></p>

<ul>
	<li><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><span style="font-size:12.0pt">J. Sedmidubsky, P. Elias, P. Zezula, "Benchmarking Search and Annotation in Continuous Human Skeleton Sequences", ICMR, 2019.</span></span></span></li>
</ul>

<p>&nbsp;</p>

<p><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><strong><span style="font-size:12.0pt">8.2.&nbsp; </span></strong><strong><span style="font-size:12.0pt">AUTH UAV Gesture Dataset</span></strong><span style="font-size:12.0pt"> (</span><a href="https://aiia.csd.auth.gr/auth-uav-gesture-dataset/" style="color:blue; text-decoration:underline" target="_blank"><u><span style="font-size:12.0pt"><span style="color:blue">https://aiia.csd.auth.gr/auth-uav-gesture-dataset/</span></span></u> </a><span style="font-size:12.0pt">).</span></span></span></p>

<ul>
	<li><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><span style="font-size:12.0pt">F. Patrona, I. Mademlis, I. Pitas, “An Overview of Hand Gesture Languages for Autonomous UAV Handling”, in Proceedings of the Workshop on Aerial Robotic Systems Physically Interacting with the Environment (AIRPHARO), 2021</span><span style="font-size:12.0pt">.</span></span></span></li>
</ul>

<p><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><span style="font-size:12.0pt"><span style="background-color:yellow">You can request for the relevant 4-class sub-dataset of the NTU RGB+D dataset using the the same request form and download from Section 3.0 of the Download Page</span></span></span></span></p>

<p><br>
<br>
Note: Users of these derived works should also cite the papers found here (see Section 7 on Related Publications)</p></div>
        <div class="row justify-content-center">
            
            <a class="btn btn-info btn-lg btn-block m-3" role="button" href="/dataset/actionRecognition/request">Request (Account Required)</a>
            
        </div>
    </div>

    </main>

    <!-- Option 1: jQuery and Bootstrap Bundle (includes Popper) -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-Piv4xVNRyMGpqkS2by6br4gNJ7DXjqk09RmUpJ8jgGtD7zP9yug3goQfGII0yAns" crossorigin="anonymous"></script>


</body></html>