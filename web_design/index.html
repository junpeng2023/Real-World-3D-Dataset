<html lang="en">

<head>
    <meta charset="UTF-8">  
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit= no">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css",integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
    <title>Dataset topic</title>
	<style>
        pre {
            background-color: #d7d8d8d8;   
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            padding: 16px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            display: block;
            width: 100%;
            white-space: pre;
            border: 1px solid #e1e4e8;
        }
    </style>
	<style>
		.myTitle {
			font-size: 24px;
			color: rgb(8, 8, 10);
			font-family: "Arial", sans-serif;
			letter-spacing: 2px;
		}
		</style>
</head>

<body>
   
    <!-- <div class="container mt-3 mb-2">
        <img src="/static/home/logo.png" alt="">
    </div>
    <header class="site-header">
        <nav class="navbar navbar-expand-md navbar-dark bg-steel pt-0 pb-0">
            <div class="container">
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle" aria-controls="navbarToggle" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarToggle">
                    <div class="navbar-nav mr-auto">
                        <a class="nav-item nav-link" href="/">Home</a>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Event</a>
                            <div class="dropdown-menu">
                                
                                <a class="dropdown-item" href="/event/VISVA2014/">VISVA 2014</a>
                                
                                <a class="dropdown-item" href="/event/VISVA2016/">VISVA 2016</a>
                                
                                <a class="dropdown-item" href="/event/VISVA2017/">VISVA 2017</a>
                                
                                <a class="dropdown-item" href="/event/VISVA2018/">VISVA 2018</a>
                                
                                <a class="dropdown-item" href="/event/VISVA2020/">VISVA 2020</a>
                                
                            </div>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Challenge</a>
                            <div class="dropdown-menu">
                                
                                <a class="dropdown-item" href="/challenge/ActionRecognitionChallenge/">Action Recognition Challenge</a>
                                
                                <a class="dropdown-item" href="/challenge/WeMageChallenge2014/">WeMage Challenge 2014</a>
                                
                            </div>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false">Dataset</a>
                            <div class="dropdown-menu">
                                
                                <a class="dropdown-item" href="/dataset/recapturedImages/">Recaptured Images Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/videoObjectInstance/">Video Object Instance Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/actionRecognition/">Action Recognition Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/sir2Benchmark/">SIR2 Benchmark Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/faceLivenessDetection/">ROSE-Youtu Face Liveness Detection Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/cctvFights/">NTU CCTV-Fights Dataset</a>
                                
                                <a class="dropdown-item" href="/dataset/Warwick-NTU/">Warwick-NTU Multi-camera Forecasting (WNMF) database</a>
                                
                                <a class="dropdown-item" href="/dataset/SIXray-D/">SIXray-D Annotations</a>
                                
                            </div>
                        </li>
                        <a class="nav-item nav-link" href="/contact/">Contact</a>
                    </div>
                    Navbar Right Side -->
                    <!-- <div class="navbar-nav">
                        
                        <a class="nav-item nav-link" href="/login/">Login/Signup</a>
                        
                    </div>
                </div>
            </div>
        </nav>
    </header> -->

	<!-- <div id="page" class="site">
		<div class="header-wrapper">
			<div class="header" style="background: rgb(143, 150, 214); padding-top: 102px;">
													<div class="inner-header-description gridContainer">
			<div class="row header-description-row">
		<div class="col-xs col-xs-12">
			<h1 class="hero-title">
				AUTH UAV Gesture Dataset        </h1>
						<p class="header-subtitle">Artificial Intelligence &amp; Information Analysis</p>
				</div>
			</div>
		</div>
			</div>
		</div>		 -->

<table border="0" width="350px" align="center">
	<tbody>
	<tr>
		<td> 
		<center>
   
			<h1>
			<font face="verdana" style="font-size:87%">
				Multi-Tasking Daily Living Human-Object Interaction Dataset (MDLHOI dataset)
			</font>
			</h1>
		</center>
		 <!-- <div class="media-body">
        <div class="article-content"><h2>Real World 3D Dataset for Human-Robot Interaction </h2> -->
<!-- <h2><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><strong><span style="font-size:12.0pt">(</span></strong><span style="font-size:12.0pt">also include </span><strong><span style="font-size:12.0pt">AUTH UAV Gesture Dataset: NTU 4-Class)</span></strong></span></span></h2> -->

   <br>
  
<p>&nbsp;</p>
  <font face="verdana">
   <h2>1. Introduction</h2>
   <hr style="margin-top:-10px; margin-bottom:13px">
   
<p> This dataset is a collection of 120 RGB-D videos.<br>
    The 120 videos consist of <strong>8 different scenarios, namely ”working”, ”watering flowers”, ”making coffee”, "cooking", "reading", "eating", "making up" and "cutting fruits"</strong>. <br> All of which are designed to simulate the elderly's daily life under laboratory condition. <br>
     Each scene has three modes: <strong>”single person”, ”multi person” and ”cooperation”</strong>. <br>
     There are five completely random action sequence videos in each mode. <br>Compare to other existing state-of-the-art datasets, for the same scenario, we recorded <strong>not only the actions of a single person, but also recorded the collaboration between the two people</strong>, in order to improve the performance of humanoid robots.
     
     Our dataset of 120 videos is divided into <strong>a training set of 100 videos</strong> and <strong>a validation set of 20 videos </strong>.<br> The training set consists of 100 videos that are used to train our machine learning model.<br> These videos provide the model with different examples and scenarios to learn, enabling it to generalize and make accurate predictions on new data. <br>Various features such as object detection, action recognition, and scene understanding are extracted from the training videos to develop a robust model.

	 The validation set includes 20 videos that are not included in the training set. <br>These videos serve as an unbiased evaluation metric to measure the performance and generalization ability of our trained models. <br>By comparing the model's predictions on the validation set with ground truth, we can determine its accuracy, precision and recall, as well as other performance metrics. This information is critical to understanding the strengths and weaknesses of the model and guiding further improvements.


	 In addition, the annotations of the RGB videos are stored in <strong>120 json files</strong>, with the same name as the corresponding video.<br> The json file contains the information of bounding boxes of humans and objects, human-object-interactions, interacting objects and verbs.		 
	 The structure inside a json file is shown below: <br>
	 
    </p>
	
	<!-- <ul>
		<li>Parent Node
			<ul>
				<li>Child Node 1</li>
				<li>Child Node 2
					<ul>
						<li>Grandchild Node 1</li>
						<li>Grandchild Node 2</li>
					</ul>
				</li>
				<li>Child Node 3</li>
			</ul>
		</li>
		<li>Parent Node 2
			<ul>
				<li>Child Node 1</li>
				<li>Child Node 2</li>
			</ul>
		</li>
	</ul> -->

	<!-- <div style="width:75%; margin:auto;" align="left">
	
		
	<pre style="background-color: #d7d8d8d8; padding: 10px;">
	json file
		"annotation"
			"boxes_h":[[140.01024000000004, -0.00035999999994373866, 648.93056, 665.0298]]
			"boxes_o":[[601.99936, 561.01968, 719.3497600000001, 681.82992]]
			"hoi":[0]
			"interact_object":[[0,2]]
			"objects_bbox_all":[[601.99936, 561.01968, 719.3497600000001, 681.82992], [820.4492799999999, 454.78044, 931.82976, 642.4102800000001],[609.69024, 689.5396800000001, 730.49024, 720.0000000000001],[784.50048, 561.8804399999999, 867.56992, 696.3901199999999],[737.34016, 359.67959999999994, 894.12992, 573.01992], [718.49984, 583.30044, 797.31968, 717.81012],[704.7904, 463.34988, 797.32032, 636.4198799999999],[140.01024000000004, -0.00035999999994373866, 648.93056, 665.0298]]
			"objects_all_string"
			"objects_all_num"
			"verb"
		</pre>
		
	</div> -->

	
	

<div style="width:75%; margin:auto;" align="center">
	<figure style="text-align:center;">
	<img alt="" src="https://raw.githubusercontent.com/junpeng2023/Real-World-3D-Dataset/main/images/all_demo_1.png" style="width: 800px; height: auto; margin: auto;" align="center">
	
	<figcaption>proposed HOI-120 Dataset with annotations</figcaption>
</figure>
</div>

<p>&nbsp;</p>
<div style="width:75%; margin:auto;" align="left">
	
	<pre style="background-color: #d7d8d8d8; padding: 10px;">
	json file
	├── "annotation":          # annotations
		├── "boxes_h":            # bounding boxes of humans
		├── "boxes_o":            # bounding boxes of objects
		├── "hoi":                # human-object-interactions
		├── "interact_object":    # interacting objects
		├── "objects_bbox_all":   # bounding boxes of all objects
		├── "objects_all_string": # all objects
		├── "objects_all_num":    # the number of all objects
		└── "verb":               # action labels
		</pre>
		<div style="width:100%; margin:auto;" align="center">
			<p>The information each json file contains</p>
		</div>
	</div>


<h3>2. Dataset in number</h3>
<hr style="margin-top:-10px; margin-bottom:13px">
<!-- <strong>Note: actions labelled from A1 to A60 are contained in "NTU RGB+D", and actions labelled from A1 to A120 are in "NTU RGB+D 120".</strong></p> -->

<p>&nbsp;</p>


<div style="width:75%; margin:auto;">
    <p><strong><em>2.1 Some facts about the dataset</em></strong></p>

<table align="center" border="1" style="width:92%">
    <tbody>
        <tr>
            <td >Subjects</td>
            <td >8 subjects (2 female, 6 male; 7 right-handed, 1 left-handed)</td>
        </tr>
        <tr>
            <td >Senarios</td>
            <td >coffee/breakfast/reading/flower/working/eating/fruit/make_up/wear/bad</td>
        </tr>
        <tr>
            <td >Recordings</td>
            <td >120 RGB-D videos(3 groups with 5 repetitions for each senario)</td>
        </tr>
        <tr>
            <td >Playtime</td>
            <td >1 hour and 15 minutes, or 221000 RGB-D image frames</td>
        </tr>
		<tr>
            <td >mean frames</td>
            <td >221000 RGB-D image frames</td>
        </tr>
		<tr>
            <td >max frames</td>
            <td >771 RGB-D image frames</td>
        </tr>
        <tr>
            <td >Quality</td>
            <td >640 px × 480 px image resolution; 30 fps</td>
        </tr>
        <tr>
            <td >Actions</td>
            <td >27 primary actions </td>
        </tr>
        <tr>
            <td >Objects</td>
            <td >42 commonly used objects in daily life </td>
        </tr>
        <tr>
            <td >Annotations</td>
            <td >Actions and object bounding boxes are fully labeled for each frame</td>
	    </tr>
    </tbody>
</table>
</div>

<div style="clear:both;"></div>
<p>&nbsp;</p>

<div  style="width:75%; margin:auto;">
<!-- <p><strong>2.2 Action Label Mapping</strong></p> -->
<p><strong><em>2.2 Action Label Mapping</em></strong></p>


<table align="center" border="1" style="width:92%">
    <tbody>
	<tr>
	    <th>#</th>
	    <th>Action</th>
	    <th>Description</th>  
	</tr >
	<tr >
	    <td>0</td>
	    <td>idle</td>
	    <td>The human does nothing semantically meaningful.</td>
	</tr>
	<tr>
        <td>1</td>
	    <td>approach</td>
	    <td>The human approaches an object wgich is going to be relevant.</td>
	</tr>
	<tr>
	    <td >2</td>
	    <td>leave</td>
	    <td>The human leaves from an object after interacting with it.</td>
	</tr>
    <tr>
    <td >3</td>
	    <td >move</td>
	    <td >The human changes the placement of an object.</td>
	<tr>
	    <td >4</td>
	    <td >take </td>
	    <td >The human holds an object to get ready to use it.</td>
	</tr>
	<tr>
	    <td >5</td>
	    <td >place</td>
	    <td >The human places an object after using it.</td>
	</tr>
	<tr>
	    <td >6</td>
	    <td >pour</td>
	    <td >The human pours something from the grasped object.</td>
	</tr>
	<tr>
	    <td >7</td>
	    <td >cut</td>
	    <td >The human cuts something with the grasped object.</td>
	</tr>
    <tr>
	    <td >8</td>
	    <td >stir</td>
	    <td >The human stirs something with the grasped object.</td>
	</tr>
    <tr>
	    <td >9</td>
	    <td >wipe</td>
	    <td >The human wipes something with the grasped object.</td>
	</tr>
    <tr>
	    <td >10</td>
	    <td >drink</td>
	    <td >The human drinks something.</td>
	</tr>
    <tr>
	    <td >11</td>
	    <td >eat</td>
	    <td >The human eats something.</td>
	</tr>
    <tr>
	    <td >12</td>
	    <td >wear</td>
	    <td >The human wears the object on the body.</td>
	</tr>
    <tr>
	    <td >13</td>
	    <td >open</td>
	    <td >The human opens an object by hands.</td>
	</tr>
    <tr>
	    <td >14</td>
	    <td >read</td>
	    <td >The human reads an object in hand.</td>
	</tr>
    <tr>
	    <td >15</td>
	    <td >write</td>
	    <td >The human writes with the grasped object.</td>
	</tr>
    <tr>
	    <td >16</td>
	    <td >squeeze</td>
	    <td >The human squeeze something by hand.</td>
	</tr>
    <tr>
	    <td >17</td>
	    <td >smell</td>
	    <td >The human smell something.</td>
	</tr>
    <tr>
	    <td >18</td>
	    <td >close</td>
	    <td >The human closes something by hand.</td>
	</tr>
    <tr>
	    <td >19</td>
	    <td >spray</td>
	    <td >The human spray something by hand.</td>
	</tr>
    <tr>
	    <td >20</td>
	    <td >prune</td>
	    <td >The human prunes plants with sheers.</td>
	</tr>
    <tr>
	    <td >21</td>
	    <td >play</td>
	    <td >The human operate on computer or phone.</td>
	</tr>
    <tr>
	    <td >22</td>
	    <td >talk</td>
	    <td >The human speak on a phone.</td>
	</tr>
    <tr>
	    <td >23</td>
	    <td >peel</td>
	    <td >The human peels fruits with peeler.</td>
	</tr>
    <tr>
	    <td >24</td>
	    <td >look at</td>
	    <td >The human looks at something.</td>
	</tr>
     <tr>
	    <td >25</td>
	    <td >cumb</td>
	    <td >The human cumb the hair.</td>
	</tr>
     <tr>
	    <td >26</td>
	    <td >brush</td>
	    <td >The human brushes something with hand.</td>
	</tr>
</tbody>
</table>
</div>



<!-- <div style="width:75%; margin:auto;" align="center">
<p><img alt="" src="/home/ziwei/FP_plus/action_cal_1.png" style="float:none; height:auto; width: 650px;"></p>
</div> -->
<!-- <p><img alt="" src="/home/ziwei/FP_plus/three_modes.png" style="float:none; height:auto; width: 1000px;"></p> -->




<!-- <p> whitespace</p> -->


<div style="clear:both;"></div> 

<p>&nbsp;</p>
<div  style="width:85%; margin:auto;">
<p><strong><em>2.3  Object Class Label Mapping</em></strong></p>

<table align="center" border="1" style="width:92%">
	<tbody>
        
		<tr>
			<td>object 0: human1</td>
			<td>object 1: human2</td>
			<td>object 2: cup</td>
			<td>object 3: milk </td>
		</tr>
		<tr>
			<td>object 4: coffee</td>
			<td>object 5: scoop</td>
			<td>object 6: sugar</td>
			<td>object 7: kettle</td>
		</tr>
		<tr>
			<td>object 8: honig </td>
			<td>object 9: bowl</td>
			<td>object 10: tee bag</td>
			<td>object 11: cereal</td>
		</tr>
		<tr>
			<td>object 12: rug </td>
			<td>object 13: glasses</td>
			<td>object 14: pencil</td>
			<td>object 15: notebook</td>
		</tr>
		<tr>
			<td>object 16: newspaper</td>
			<td>object 17: phone</td>
			<td>object 18: flower </td>
			<td>object 19: plants </td>
		</tr>
		<tr>
			<td>object 20: container</td>
			<td>object 21: shears</td>
			<td>object 22: computer</td>
			<td>object 23: ipad </td>
		</tr>
		<tr>
			<td>object 24: place mat </td>
			<td>object 25: bread </td>
			<td>object 26: plate</td>
			<td>object 27: knife </td>
		</tr>
        <tr>
			<td>object 28: chopping block </td>
			<td>object 29: apple   </td>
			<td>object 30: banana</td>
			<td>object 31: peeler </td>
		</tr>
        <tr>
			<td>object 32: perfume </td>
			<td>object 33: mirror </td>
			<td>object 34: lipstick</td>
			<td>object 35: cumb  </td>
		</tr>
        <tr>
			<td>object 36: hut </td>
			<td>object 37: cloth </td>
			<td>object 38: bag</td>
			<td>object 39: keys </td>
		</tr>
        <tr>
			<td>object 40: toothbrush</td>
			<td>object 41: toothpaste </td>
			<td>-</td>
			<td>-</td>
		</tr>
	</tbody>
</table>
</div>

<div style="clear:both;"></div>


<p>&nbsp;</p>
<h3>3. Action Ground Truth</h3>
<hr style="margin-top:-10px; margin-bottom:13px">

<div style="clear:both;"></div>
<p>&nbsp;</p>
<div style="width:75%; margin:auto;" align="center">
	<figure style="text-align:center;">
<img alt="" src="https://raw.githubusercontent.com/junpeng2023/Real-World-3D-Dataset/main/images/action_distribution_1.png" style="float:center; height:auto; width: 500px;">
<div style="clear:both;"></div>
<p>&nbsp;</p>
<figcaption style="text-align:center;"> Action Distribution in Videos</figcaption>
</figure>
</div>
<!-- <p>following data format is used in Action ground truth:<br>

    ```python<br>

    
    ```</p> -->


<p>&nbsp;</p>
    <p>For each video human action grundtruth, we record the start and end frames of each action
		as follows, and then represent the human action and the object being touched with the
		corresponding numeric code in the middle bracket. For example, [1,17] means that the
		person approaches the phone. The action segment[...94,[22,17],263...] means the human is
		talking on a phone from frame 94 to frame 263.
		For multiple people mode as well as for cooperative mode. The actions of each person
		in the scene are labelled in turn in the middle brackets. For example, the action segment
		[21,17,5,2] means that the first person is playing with his phone while the second person
		is putting down his glass.</p>
	<div style="width:75%; margin:auto;" align="left">
	
		<figure style="text-align:center;">
		<pre style="background-color: #d7d8d8d8; padding: 10px;">
        work_single_2:[0,[0,0],42,[1,22],53,[21,22],189,[2,22],196,[1,14],206,[4,14],210,[1,15],221,[13,15]<br>,262,[15,15],463,[18,15],480,[5,14],507,[2,14],547,[0,0],564]
		</pre>
		<figcaption style="text-align:center;">Sample of action groundtruth template for single person mode</figcaption>
		</figure>
		</div>

	<div style="width:75%; margin:auto;" align="left">
	
		<figure style="text-align:left;">
		<pre style="background-color: #d7d8d8d8; padding: 10px;">
        coffee_mp_4:[0,[0,0,0,1],11,[1,2,0,1],37,[1,2,1,2],40,[4,2,1,2]<br>,42,[4,2,4,2],64,[5,2,5,2],66,[5,2,1,5],71,[5,2,4,5],75,[2,2,8,5],83,[1,4,2,2],96,[1,4,1,2],101,[4,4,8,5],123,[6,4,8,5]
		,135,[6,4,5,5],140,[6,4,2,5],151,[6,4,1,3],161,[6,4,4,3],181,[3,4,4,3]<br>,196,[5,4,5,3],200,[5,4,2,3],211,[5,4,1,4],215,[6,4,1,4]],222,[2,4,4,4],232,[1,7,4,4],236,[1,7,5,4]
		,261,[4,7,4,4],273,[4,7,6,4],307,[6,7,5,4],334,[6,7,2,4],347,[6,7,1,5]<br>,357,[5,7,4,5],368,[5,7,2,5],378,[2,7,2,5],386,[2,7,1,7],399,[1,6,4,7],413,[1,3,4,7],424,[4,3,6,7]
		,456,[6,3,6,7],475,[6,3,5,2],503,[5,3,3,7],523,[5,2,5,7],537,[2,3,5,7]<br>,544,[1,2,2,7],557,[4,5,2,7],568,[8,5,4,5],576,[8,5,8,5],615,[8,5,5,5],617,[8,5,2,2],627,[8,5,1,6]
		,634,[5,5,1,6],641,[2,5,4,6],655,[1,3,4,6],664,[4,3,4,6],679,[4,3,6,6]<br>,685,[5,3,6,6],700,[2,3,6,6],717,[1,2,4,6],721,[1,2,5,6],732,[4,2,5,6],741,[5,2,5,6],747,[5,2,2,6]
		,758,[2,2,8,5],776,[0,0,8,5],795,[0,0,5,5],810,[0,0,2,5],819,[0,0,1,3]<br>,825,[0,0,3,3],837,[0,0,2,3],843,[0,0,1,3],846,[0,0,4,3],856,[0,0,6,3],881,[0,0,4,3],897,[0,0,5,3]
		,912,[0,0,2,3],921,[0,0,1,5],928,[0,0,4,5],935,[0,0,8,5],966,[0,0,5,5]<br>,970,[0,0,2,5],992,[0,0,4,2],1006,[0,0,10,2],1025,[0,0,5,2],1052,[0,0,2,2],1062,[0,0,0,1],1085]
		</pre>
		<figcaption style="text-align:center;">Sample of action groundtruth template for multi person mode</figcaption>
		</figure>
		</div>

	<div style="width:75%; margin:auto;" align="left">
	
		<figure style="text-align:left;">
		<pre style="background-color: #d7d8d8d8; padding: 10px;">
        work_col_2:[0,[0,0,0,1],24,[1,22,0,1],34,[21,22,0,1],115,[21,22,1,17]<br>,124,[14,22,1,17],145,[14,22,4,17],166,[1,17,5,17],182,[4,17,2,17],200,[4,17,0,1],211,[22,17,0,1],365,[22,17,1,2]
		,392,[22,17,4,2],412,[5,17,5,2],432,[2,17,5,2],437,[1,2,2,2],463,[4,2,0,1]<br>,490,[10,2,0,1],555,[5,2,0,1],586,[2,2,0,1],603,[0,0,0,1]
		,613,[0,0,1,2],644,[0,0,3,2],677,[0,0,2,2],697,[0,0,0,1],725]
		</pre>
		<figcaption style="text-align:center;">Sample of action groundtruth template for coorperation mode</figcaption>
		</figure>
		</div>






<p>&nbsp;</p>

<!-- <h3>3. RGB-D Camera Setup</h3>

<p>We use Intel RealSense Depth Camera D435i as RGB-D camera to record data.<br>The Intel® RealSense™ D435i places an IMU into our cutting‑edge stereo depth camera. With an Intel module and vision processor in a small form factor, the D435i is a powerful  complete package which can be paired with customizable software for a depth camera that is capable of understanding it's own movement.</p>

<p>&nbsp;</p> -->

<!-- <h3>4. Guidline to Data recording</h3>

<pre>
    <code>
    roscore
    
    roslaunch realsense2_camera rs_camera.launch align_depth:=true<br> depth_width:=640 depth_height:=480 depth_fps:=30 color_width:=640 color_height:=480 color_fps:=30 
    
    rosbag record /camera/accel/imu_info /camera/accel/sample /camera/align_to_color/parameter_descriptions <br>/camera/align_to_color/parameter_updates /camera/aligned_depth_to_color/camera_info /camera/aligned_depth_to_color/image_raw<br> /camera/color/camera_info /camera/color/image_raw/compressed /camera/extrinsics/depth_to_color<br> /camera/extrinsics/depth_to_infra1 /tf /tf_static -o breakfast.bag
    </code>
      </pre> 

<p>&nbsp;</p> -->

<p>&nbsp;</p>

<!-- <h3>5. Contributing</h3>

<p>State if you are open to contributions and what your requirements are for accepting them.<br>

    For people who want to make changes to your project, it's helpful to have some documentation on how to get started. Perhaps there is a script that they should run or some environment variables that they need to set. 
    Make these steps explicit. These instructions could also be useful to your future self.<br>
    
    You can also document commands to lint the code or run tests. These steps help to ensure high code quality and reduce the likelihood that the changes inadvertently break something. Having instructions for running tests is especially helpful if it requires external setup, such as starting a Selenium server for testing in a browser..</p>

<p>&nbsp;</p> -->

<!-- <h3>. Contributing</h3>

<p>State if you are open to contributions and what your requirements are for accepting them.<br>

    For people who want to make changes to your project, it's helpful to have some documentation on how to get started. Perhaps there is a script that they should run or some environment variables that they need to set. 
    Make these steps explicit. These instructions could also be useful to your future self.<br>
    
    You can also document commands to lint the code or run tests. These steps help to ensure high code quality and reduce the likelihood that the changes inadvertently break something. Having instructions for running tests is especially helpful if it requires external setup, such as starting a Selenium server for testing in a browser..</p>

<p>&nbsp;</p> -->

<!-- <h3>5. Statistics</h3> -->

<!-- <p>We provide more information about the data, answers to FAQs, samples codes to read the data, and the latest published results on our datasets <a href="https://github.com/shahroudy/NTURGB-D" target="_blank">here</a>.</p> -->

<!-- <p> If you want more information about the dataset, answers to FAQs and sample codes to read the data, more details can be found <a href=""></a> </p>
<p>&nbsp;</p> -->


<h3>4. Dataset Size</h3>
<hr style="margin-top:-10px; margin-bottom:13px">

<p>To simplify the download process, we list the size of each kind of file, which is shown in the table below</p>

<table align="center" border="1" style="width:55%">
	<tbody>
		<tr>
			<td><strong>File Types</strong></td>
			<td><strong>"Real World 3D Dataset"</strong></td>
			
		</tr>
		<tr>
			<td>The 120 json files</td>
			<td>86.49 MB</td>
			
		</tr>
		<tr>
			<td>rosbag files</td>
			<td>260.1 GB</td>
			
		</tr>
		<tr>
			<td>Depth videos</td>
			<td> 3.0 GB</td>
			
		</tr>
		<tr>
			<td>RGB videos</td>
			<td> 2.9 GB</td>
			
		</tr>
		<tr>
			<td>Yolo outputs</td>
			<td> 177.8 GB</td>
			
		</tr>
		<tr>
			<td>png files for pictures</td>
			<td> 5.0 GB</td>
			
		</tr> 
		
		<tr>
			<td><strong>Total</strong></td>
			<td><strong>448.89 GB</strong></td>
			
		</tr>
	</tbody>
</table>

<!-- <script>
	var data = {"annotation": [{"boxes_h": [[140.01024000000004, -0.00035999999994373866, 648., 648.93056, 665.0298]], "boxes_o": [[601.99936, 561.01968, 719.3497600000001, 681.82992]], "hoi": [0], "interact_object": [[0, 2]], "objects_bbox_allox_all": [[601.99936, 561.01968, 719.3497600000001, 681.82992], [820.4492799992799999999, 454.78044, 931.82976, 642.4102800000001], [609.69024, 689.53968000  -0.00035999999994373866, 648.93056, 665.0298]], "objects_all_string": ["cup""cup", "milk", "scoop", "sugar", "kettle", "honey", "coffee", "human1"], "objects_all_num": [2, 3, 5, 6, 7, 8, 4, 0], "verb": [4]}]};
	
	function createTree(container, obj) {
		container.innerHTML = createTreeText(obj, 0);
	}
	
	function createTreeText(obj, nest) {
		if (typeof obj !== 'object') return obj;
		
		var txt = '';
		var nestStr = '&nbsp;&nbsp;'.repeat(nest * 2);
		
		for (var key in obj) {
			txt += nestStr + key + '<br>';
			txt += createTreeText(obj[key], nest + 1);
		}
		
		return txt;
	}
	
	createTree(document.getElementById('jsonTree'), data);
	
	</script> -->
	

<p>&nbsp;</p>

<!-- <p>*Masked depth maps are the foreground masked version of the depth maps. Masking is done based on the locations of the detected body joints, to remove the background and less important parts of the depth maps and to improve the compression rate.</p> -->

<p>&nbsp;</p>


<div style="clear:both;"></div>

<h3>5. Sample Videos</h3>
<hr style="margin-top:-10px; margin-bottom:13px">
<p>&nbsp;</p>

<figure style="text-align:center;">
    <img alt="" src="https://raw.githubusercontent.com/junpeng2023/Real-World-3D-Dataset/main/images/3_modes_demo.png" style="width: 800px; height: auto; margin: auto;">
	<figcaption>The image illustrates the examples of single person, multi-person, and coorperation HOI in computer vision.</figcaption>
    
</figure>


<p>&nbsp;</p>

<table align="center">
	<tbody>
		<tr>
			
			<!-- <td><iframe frameborder="0" height="200" src="/media/ziwei/yuankai/RGB_video/col/col_coffee/coffee_two33_2022-10-17-18-26-34_camera_color_image_raw_compressed.mp4" width="320"></iframe>​​</td> -->
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/BpOdzsZ9_us?ref=0&autoplay=1&mute=1&loop=1&playlist=BpOdzsZ9_us&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/97uPlUsHBuU?ref=0&autoplay=1&mute=1&loop=1&playlist=97uPlUsHBuU&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
		    <!-- <td><iframe frameborder="0" height="200" src="/media/ziwei/yuankai/RGB_video/col/col_makeup/make_up_col_1_2022-11-11-20-39-21_camera_color_image_raw_compressed.mp4" width="320"></iframe></td> -->
			<!-- <td><iframe frameborder="0" height="200" src="/media/ziwei/yuankai/RGB_video/single/eating/essen_one_2022-10-17-18-55-41_camera_color_image_raw_compressed.mp4" width="320"></iframe></td> -->
			<!-- <td><iframe frameborder="0" height="200" src="/media/ziwei/yuankai/RGB_video/single/eating/essen_one_2022-10-17-18-55-41_camera_color_image_raw_compressed.mp4" width="320"></iframe></td> -->
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/d-LnoEUvV00?ref=0&autoplay=1&mute=1&loop=1&playlist=d-LnoEUvV00&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/4o8N0TbprgE?ref=0&autoplay=1&mute=1&loop=1&playlist=4o8N0TbprgE&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
		</tr>
		<tr>
			
			<!-- <td><iframe frameborder="0" height="200" src="/media/ziwei/yuankai/RGB_video/col/col_coffee/coffee_two33_2022-10-17-18-26-34_camera_color_image_raw_compressed.mp4" width="320"></iframe>​​</td> -->
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/muJDg4ttx0k?ref=0&autoplay=1&mute=1&loop=1&playlist=muJDg4ttx0k&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
		    <!-- <td><iframe frameborder="0" height="200" src="/media/ziwei/yuankai/RGB_video/col/col_makeup/make_up_col_1_2022-11-11-20-39-21_camera_color_image_raw_compressed.mp4" width="320"></iframe></td> -->
			<!-- <td><iframe frameborder="0" height="200" src="/media/ziwei/yuankai/RGB_video/single/eating/essen_one_2022-10-17-18-55-41_camera_color_image_raw_compressed.mp4" width="320"></iframe></td> -->
			<!-- <td><iframe frameborder="0" height="200" src="/media/ziwei/yuankai/RGB_video/single/eating/essen_one_2022-10-17-18-55-41_camera_color_image_raw_compressed.mp4" width="320"></iframe></td> -->
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/2-lCD8plQIo?ref=0&autoplay=1&mute=1&loop=1&playlist=2-lCD8plQIo&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/GrDbe-fCmkM?ref=0&autoplay=1&mute=1&loop=1&playlist=GrDbe-fCmkM&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/VeN0CdQ_fSM?ref=0&autoplay=1&mute=1&loop=1&playlist=VeN0CdQ_fSM&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
		</tr>

		<tr>
			<!-- <td><iframe frameborder="0" height="200" src="/home/ziwei/Videos/cvat_coffee_col_3_1.mp4" width="320"></iframe>​​</td> -->
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/wTmtI949Svs?ref=0&autoplay=1&mute=1&loop=1&playlist=wTmtI949Svs&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/u882dZqjRy8?ref=0&autoplay=1&mute=1&loop=1&playlist=u882dZqjRy8&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
		
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/4qYHs27sfdI?ref=0&autoplay=1&mute=1&loop=1&playlist=4qYHs27sfdI&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
			<td><iframe frameborder="0" height="150" src="https://www.youtube.com/embed/zzSZNyb7Rv4?ref=0&autoplay=1&mute=1&loop=1&playlist=zzSZNyb7Rv4&controls=0&showinfo=0" width="240" allow="autoplay"></iframe></td>
		</tr>
        

        <!-- <tr>
			<td colspan="2">
				<video width="640" height="360" controls>
					<source src="/media/ziwei/yuankai/dataset/demo_file/reading1_2022-09-08-15-31-12_camera_color_image_raw.mp4" type="video/mp4">
					Your browser does not support the video tag.
				</video>
			</td>
		</tr> -->
	</tbody>
</table>

<h3>6. How to download</h3>
<hr style="margin-top:-10px; margin-bottom:13px">

<!-- <p>We provide more information about the data, answers to FAQs, samples codes to read the data, and the latest published results on our datasets <a href="https://github.com/shahroudy/NTURGB-D" target="_blank">here</a>.</p> -->

<p> If you want more information about the dataset, answers to FAQs and sample codes to read the data, more details can be found <a href=""></a> </p>
<p>&nbsp;</p>

<div style="margin-left: 10px; display: inline-block;">
<a href="https://github.com/junpeng2023/MDLHOI/tree/main/json_120">The 120 json files</a> 86.49 MB<br>

<a href="https://drive.google.com/open?id=1Qc3SOXdjzVUd1LDuG0ZS7DsE4Kb0FYVr">rosbag files</a> 260.1 GB<br>

<a href="https://github.com/junpeng2023/MDLHOI/tree/main/">Depth videos</a> 3.0 GB<br>

<a href="https://github.com/junpeng2023/MDLHOI/tree/main/">RGB videos</a> 2.9 GB<br>

<a href="https://drive.google.com/open?id=1Qc3SOXdjzVUd1LDuG0ZS7DsE4Kb0FYVr">Yolo outputs</a> 177.8 GB<br>

<a href="https://drive.google.com/open?id=1Qc3SOXdjzVUd1LDuG0ZS7DsE4Kb0FYVr">png files for pictures</a> 5.0 GB<br>

</div>







<h3>7. Terms &amp; Conditions of Use</h3>
<hr style="margin-top:-10px; margin-bottom:13px">

<p>This dataset is intended exclusively <strong> for academic research purposes </strong>and is available <strong>free of charge to researchers from educational or research institutions</strong>, provided it is used non-commercially.</p>

<p>Please note the following conditions that govern the use of this dataset:<br>
	• Without explicit permission from us, any redistribution, derivation or creation of a new dataset from this existing dataset, or any form of commercial usage, either partially or in its entirety, is strictly prohibited and may be considered unlawful.<br>
	• In consideration of privacy, images of any subjects included in these datasets are permitted for demonstration purposes only within academic publications and presentations.<br>
	• All users of this dataset agree to protect, defend and hold harmless, along with its officers, employees, and agents, collectively and individually, from all losses, costs, and damages.  </p>

<!-- <p><span style="font-size:11pt"><span style="font-family:Calibri,sans-serif"><span style="color:red">If interested, researchers can register for an account, submit the request form and accept the Release Agreement. We will validate your request and grant approval&nbsp;for downloading the datasets.The LoginID can be used for both "NTU RGB+D" and "NTU RGB+D 120".</span></span></span></p> -->

<!-- <h3>7. <u>Related Publications</u></h3>

<p>All publications using "Real word 3D Dataset for Human-Robot Interaction" or any of the derived datasets(see Section 8) should include the following acknowledgement: "(Portions of) the research in this paper used "Real word 3D Dataset for Human-Robot Interaction" made available by "Chair of Media Technology, Technical University of Munich""<br>
<br>
<strong>Furthermore, these publications should cite the following papers:</strong></p>

<ul>
	<li><strong>Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang, "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis", IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016</strong> <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.pdf">[PDF]</a>.</li>
	<li><strong>Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, Alex C. Kot, "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding", IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019.</strong> <a href="https://arxiv.org/pdf/1905.04757.pdf">[PDF]</a>.</li>
</ul>

<p>&nbsp;</p>

<p>Some related works:</p> -->

<!-- <ul>
	<li>Amir Shahroudy, Tian-Tsong Ng, Qingxiong Yang, Gang Wang, "Multimodal Multipart Learning for Action Recognition in Depth Videos", TPAMI, 2016.</li>
	<li>Amir Shahroudy, Tian-Tsong Ng, Yihong Gong, Gang Wang, "Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos" TPAMI, 2018.</li>
	<li>Amir Shahroudy, Gang Wang, Tian Tsong Ng, "Multi-modal Feature Fusion for Action Recognition in RGB-D Sequences", ISCCSP, 2014.</li>
	<li>Jun Liu, Amir Shahroudy, Dong Xu, Gang Wang, "Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition", ECCV, 2016.</li>
	<li>Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, Alex C. Kot, "Global Context-Aware Attention LSTM Networks for 3D Action Recognition", CVPR, 2017.</li>
	<li>Jun Liu, Amir Shahroudy, Dong Xu, Alex C. Kot, Gang Wang, "Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates", TPAMI, 2018.</li>
	<li>Jun Liu, Gang Wang, Ling-Yu Duan, Kamila Abdiyeva, Alex C. Kot, "Skeleton-Based Human Action Recognition with Global Context-aware Attention LSTM Networks", TIP, 2018.</li>
	<li>Jun Liu, Amir Shahroudy, Gang Wang, Ling-Yu Duan, Alex C. Kot, "Skeleton-Based Online Action Prediction Using Scale Selection Network", TPAMI, 2019.</li>
	<li>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, and Alex Kot, "Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-Order Feature Analysis", ECCV 2020.</li>
	<li>Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, and Alex Kot, "Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning", ICCV 2021.<br>
	&nbsp;</li>
</ul> -->


<!--need to work on-->
<!-- <ul>
    <li>Dohyung Kim, Jinhyeok Jang, Minsu Jang, Jaeyeon Lee,Jaehong Kim, "3D Daily Activity Recognition Dataset for Elderly-care Robots", Journal/Conference, Year.</li>
    <li>H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, T. Serre, "HMDB: A Large Video Database for Human Motion Recognition", Journal/Conference, Year.</li>
    <li>Christian R. G. Dreher, Mirko Wächter, Tamim Asfour, "Learning Object-Action Relations from Bimanual Human Demonstrations", Journal/Conference, Year.</li>
    <li>Rui Dai, Srijan Das, Saurav Sharma, Luca Minciullo, Lorenzo Garattoni, "Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection", Journal/Conference, Year.</li>
    <li>Meng-Jiun Chiou, Chun-Yu Liao, Li-Wei Wang, "ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos", Journal/Conference, Year.</li>
  </ul>

<h3>8. Derived Works Based on Real World 3D Dataset</h3>

<p>Below are some datasets that are derived from, or make partial use of, NTU RGB+D dataset:</p>

<p><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><strong><span style="font-size:12.0pt">8.1.&nbsp; LSMB19: A Large-Scale Motion Benchmark for Searching and Annotating in Continuous Motion Data Streams</span></strong><span style="font-size:12.0pt"> (</span><a href="http://mocap.fi.muni.cz/LSMB" style="color:blue; text-decoration:underline" target="_blank"><span style="font-size:12.0pt">http://mocap.fi.muni.cz/LSMB</span></a><span style="font-size:12.0pt">).</span></span></span></p>

<ul>
	<li><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><span style="font-size:12.0pt">J. Sedmidubsky, P. Elias, P. Zezula, "Benchmarking Search and Annotation in Continuous Human Skeleton Sequences", ICMR, 2019.</span></span></span></li>
</ul>

<p>&nbsp;</p>

<p><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><strong><span style="font-size:12.0pt">8.2.&nbsp; </span></strong><strong><span style="font-size:12.0pt">AUTH UAV Gesture Dataset</span></strong><span style="font-size:12.0pt"> (</span><a href="https://aiia.csd.auth.gr/auth-uav-gesture-dataset/" style="color:blue; text-decoration:underline" target="_blank"><u><span style="font-size:12.0pt"><span style="color:blue">https://aiia.csd.auth.gr/auth-uav-gesture-dataset/</span></span></u> </a><span style="font-size:12.0pt">).</span></span></span></p>

<ul>
	<li><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><span style="font-size:12.0pt">F. Patrona, I. Mademlis, I. Pitas, “An Overview of Hand Gesture Languages for Autonomous UAV Handling”, in Proceedings of the Workshop on Aerial Robotic Systems Physically Interacting with the Environment (AIRPHARO), 2021</span><span style="font-size:12.0pt">.</span></span></span></li>
</ul>

<p><span style="font-size:11pt"><span style="font-family:&quot;Calibri&quot;,sans-serif"><span style="font-size:12.0pt"><span style="background-color:yellow">You can request for the relevant 4-class sub-dataset of the NTU RGB+D dataset using the the same request form and download from Section 3.0 of the Download Page</span></span></span></span></p>

<p><br>
<br>
Note: Users of these derived works should also cite the papers found here (see Section 7 on Related Publications)</p></div>
        <div class="row justify-content-center">
            
            <a class="btn btn-info btn-lg btn-block m-3" role="button" href="/dataset/actionRecognition/request">Request (Account Required)</a>
            
        </div>
    </div>

    </main> -->

    <!-- Option 1: jQuery and Bootstrap Bundle (includes Popper) -->
    <!-- <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-Piv4xVNRyMGpqkS2by6br4gNJ7DXjqk09RmUpJ8jgGtD7zP9yug3goQfGII0yAns" crossorigin="anonymous"></script> -->
	</td>
</tr>
</tbody>
</table> 

</font>
</body>

</html>